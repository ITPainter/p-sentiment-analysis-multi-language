# -*- coding: utf-8 -*-
"""
ğŸ‡¨ğŸ‡³ ì¤‘êµ­ì–´ ê°ì„±ë¶„ì„ (Chinese Sentiment Analysis)
Google Colabì—ì„œ ì‹¤í–‰ ê°€ëŠ¥í•œ ì¤‘êµ­ì–´ í…ìŠ¤íŠ¸ ê°ì„±ë¶„ì„ ì‹œìŠ¤í…œ

ğŸ“‹ ê¸°ëŠ¥:
- Excel íŒŒì¼ì—ì„œ ì¤‘êµ­ì–´ í…ìŠ¤íŠ¸ ì½ê¸°
- ì¤‘êµ­ì–´ ì „ìš© ê°ì„±ë¶„ì„ ëª¨ë¸ ì‚¬ìš©
- ê²°ê³¼ë¥¼ Excel íŒŒì¼ë¡œ ì €ì¥

ğŸš€ ì‚¬ìš©ë²•:
1. Excel íŒŒì¼ì„ ì—…ë¡œë“œ (í…ìŠ¤íŠ¸ ì»¬ëŸ¼ í¬í•¨)
2. ì½”ë“œ ì‹¤í–‰
3. ê²°ê³¼ ë‹¤ìš´ë¡œë“œ
"""

# í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜
# !pip install torch transformers pandas openpyxl matplotlib seaborn wordcloud jieba

import torch
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from wordcloud import WordCloud
import warnings
import logging
from google.colab import files
from io import BytesIO
import re
import jieba

# ê²½ê³  ë©”ì‹œì§€ ì°¨ë‹¨
warnings.filterwarnings('ignore')
logging.getLogger('transformers').setLevel(logging.ERROR)

# ì¤‘êµ­ì–´ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = ['DejaVu Sans', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

print("âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ")

def load_chinese_model():
    """ì¤‘êµ­ì–´ ê°ì„±ë¶„ì„ ëª¨ë¸ ë¡œë“œ"""
    # ì¤‘êµ­ì–´ ê°ì„±ë¶„ì„ ëª¨ë¸ (ê°ì„±ë¶„ì„ ì „ìš©)
    model_name = "IDEA-CCNL/Erlangshen-RoBERTa-110M-Sentiment"
    
    print(f"ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}")
    
    # í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(model_name)
    
    # GPU ì‚¬ìš© ê°€ëŠ¥ì‹œ GPU ì‚¬ìš©
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    
    print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! ë””ë°”ì´ìŠ¤: {device}")
    print(f"ğŸ“Š ëª¨ë¸: {model_name}")
    
    return tokenizer, model, device

def upload_excel_file():
    """Excel íŒŒì¼ ì—…ë¡œë“œ"""
    print("ğŸ“ Excel íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”...")
    uploaded = files.upload()
    
    if not uploaded:
        print("âŒ íŒŒì¼ì´ ì—…ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return None, None
    
    # ì²« ë²ˆì§¸ íŒŒì¼ ì‚¬ìš©
    filename = list(uploaded.keys())[0]
    print(f"âœ… íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ: {filename}")
    
    # Excel íŒŒì¼ ì½ê¸°
    try:
        df = pd.read_excel(BytesIO(uploaded[filename]))
        print(f"ğŸ“Š Excel íŒŒì¼ ì½ê¸° ì„±ê³µ: {len(df)} í–‰, {len(df.columns)} ì»¬ëŸ¼")
        print(f"ğŸ“‹ ì»¬ëŸ¼ëª…: {list(df.columns)}")
        print("\nğŸ” ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°:")
        print(df.head())
        return df, filename
    except Exception as e:
        print(f"âŒ Excel íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {str(e)}")
        return None, None

def find_text_column(df):
    """í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì°¾ê¸°"""
    text_columns = ['comment', 'è¯„è®º', 'review', 'text', 'comments', 'æ–‡æœ¬', 'å†…å®¹', 'ç•™è¨€']
    text_column = None
    
    for col in df.columns:
        if col.lower() in [tc.lower() for tc in text_columns]:
            text_column = col
            break
    
    if text_column is None:
        # ì»¬ëŸ¼ ë‚´ìš©ìœ¼ë¡œ ì¶”ì •
        for col in df.columns:
            if df[col].dtype == 'object':
                sample_data = df[col].dropna().head(5)
                if len(sample_data) > 0:
                    avg_length = sample_data.astype(str).str.len().mean()
                    if avg_length > 5:  # ì¤‘êµ­ì–´ëŠ” ì§§ì•„ë„ ì˜ë¯¸ìˆìŒ
                        text_column = col
                        break
    
    if text_column:
        print(f"âœ… í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì„ íƒ: {text_column}")
        print(f"ğŸ“ í…ìŠ¤íŠ¸ ê°œìˆ˜: {len(df[text_column].dropna())}")
        print("\nğŸ” í…ìŠ¤íŠ¸ ìƒ˜í”Œ:")
        for i, text in enumerate(df[text_column].dropna().head(3)):
            print(f"{i+1}. {text[:100]}{'...' if len(str(text)) > 100 else ''}")
    else:
        print("âŒ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼:")
        for col in df.columns:
            print(f"- {col} ({df[col].dtype})")
    
    return text_column

def analyze_sentiment(texts, tokenizer, model, device):
    """ì¤‘êµ­ì–´ í…ìŠ¤íŠ¸ ê°ì„±ë¶„ì„"""
    results = []
    batch_size = 16
    
    # ë¼ë²¨ ì •ì˜ (ì¤‘êµ­ì–´ ëª¨ë¸ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
    labels = ["negative", "neutral", "positive"]
    
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i+batch_size]
        
        # í† í¬ë‚˜ì´ì§•
        inputs = tokenizer(batch_texts, padding=True, truncation=True, 
                          return_tensors="pt", max_length=128).to(device)
        
        with torch.no_grad():
            outputs = model(**inputs)
            logits = outputs.logits
            probs = torch.softmax(logits, dim=-1)
            
            for j, prob in enumerate(probs):
                max_idx = torch.argmax(prob).item()
                confidence = prob[max_idx].item()
                
                results.append({
                    'text': batch_texts[j],
                    'sentiment_label': labels[max_idx],
                    'sentiment_confidence': confidence,
                    'negative_prob': prob[0].item(),
                    'neutral_prob': prob[1].item(),
                    'positive_prob': prob[2].item()
                })
    
    return results

def create_visualizations(results_df):
    """ì‹œê°í™” ìƒì„±"""
    if results_df.empty:
        print("âŒ ì‹œê°í™”í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # 1. ê°ì„± ë¶„í¬ ë§‰ëŒ€ ì°¨íŠ¸
    plt.figure(figsize=(10, 6))
    sentiment_counts = results_df['sentiment_label'].value_counts()
    colors = ['#F44336', '#9E9E9E', '#4CAF50']  # ë¹¨ê°•, íšŒìƒ‰, ì´ˆë¡
    
    bars = plt.bar(sentiment_counts.index, sentiment_counts.values, color=colors, alpha=0.8)
    plt.title('ì¤‘êµ­ì–´ í…ìŠ¤íŠ¸ ê°ì„±ë¶„ì„ ê²°ê³¼', fontsize=16, fontweight='bold')
    plt.xlabel('ê°ì„±', fontsize=12)
    plt.ylabel('í…ìŠ¤íŠ¸ ìˆ˜', fontsize=12)
    
    # ê°’ í‘œì‹œ
    for bar, count in zip(bars, sentiment_counts.values):
        plt.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.5,
                str(count), ha='center', va='bottom', fontweight='bold')
    
    plt.tight_layout()
    plt.show()
    
    # 2. ê°ì„± ë¶„í¬ íŒŒì´ ì°¨íŠ¸
    plt.figure(figsize=(8, 8))
    plt.pie(sentiment_counts.values, labels=sentiment_counts.index, autopct='%1.1f%%',
            colors=colors, startangle=90)
    plt.title('ê°ì„± ë¶„í¬ ë¹„ìœ¨', fontsize=16, fontweight='bold')
    plt.axis('equal')
    plt.show()
    
    # 3. ì‹ ë¢°ë„ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨
    plt.figure(figsize=(10, 6))
    plt.hist(results_df['sentiment_confidence'], bins=20, alpha=0.7, color='orange')
    plt.title('ê°ì„±ë¶„ì„ ì‹ ë¢°ë„ ë¶„í¬', fontsize=16, fontweight='bold')
    plt.xlabel('ì‹ ë¢°ë„', fontsize=12)
    plt.ylabel('ë¹ˆë„', fontsize=12)
    
    # í‰ê· ì„  ì¶”ê°€
    mean_confidence = results_df['sentiment_confidence'].mean()
    plt.axvline(mean_confidence, color='red', linestyle='--', 
                label=f'í‰ê· : {mean_confidence:.3f}')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()
    
    # 4. ì›Œë“œí´ë¼ìš°ë“œ (ê¸ì •ì  í…ìŠ¤íŠ¸ë§Œ)
    positive_texts = results_df[results_df['sentiment_label'] == 'positive']['text'].tolist()
    if positive_texts:
        combined_text = ' '.join(positive_texts)
        
        # ì¤‘êµ­ì–´ í† í°í™” ë° ì „ì²˜ë¦¬
        try:
            # jiebaë¡œ ì¤‘êµ­ì–´ í† í°í™”
            words = []
            for text in positive_texts:
                words.extend(jieba.cut(text))
            
            processed_text = ' '.join(words)
            
            if processed_text.strip():
                wordcloud = WordCloud(
                    width=800, height=400,
                    background_color='white',
                    max_words=100,
                    min_font_size=12,
                    relative_scaling=0.5,
                    colormap='viridis'
                ).generate(processed_text)
                
                plt.figure(figsize=(16, 8))
                plt.imshow(wordcloud, interpolation='bilinear')
                plt.axis('off')
                plt.title('ê¸ì •ì  í…ìŠ¤íŠ¸ ì›Œë“œí´ë¼ìš°ë“œ', fontsize=20, fontweight='bold')
                plt.tight_layout()
                plt.show()
        except Exception as e:
            print(f"ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì‹¤íŒ¨: {str(e)}")

def save_results(df, results_df, original_filename):
    """ê²°ê³¼ ì €ì¥"""
    if results_df.empty:
        print("âŒ ì €ì¥í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì›ë³¸ ë°ì´í„°ì™€ ê²°ê³¼ í•©ì¹˜ê¸°
    final_df = df.copy()
    
    # ê²°ê³¼ ì»¬ëŸ¼ ì¶”ê°€
    for col in results_df.columns:
        if col != 'text':  # text ì»¬ëŸ¼ì€ ì œì™¸ (ì¤‘ë³µ)
            final_df[col] = results_df[col]
    
    # ê²°ê³¼ íŒŒì¼ëª… ìƒì„±
    output_filename = f"chinese_sentiment_analysis_result_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
    
    # Excel íŒŒì¼ë¡œ ì €ì¥
    with pd.ExcelWriter(output_filename, engine='openpyxl') as writer:
        # ì „ì²´ ê²°ê³¼
        final_df.to_excel(writer, sheet_name='æƒ…æ„Ÿåˆ†æ_ç»“æœ', index=False)
        
        # ê°ì„±ë³„ ìš”ì•½
        sentiment_summary = results_df['sentiment_label'].value_counts().reset_index()
        sentiment_summary.columns = ['æƒ…æ„Ÿ', 'æ•°é‡']
        sentiment_summary['æ¯”ä¾‹(%)'] = (sentiment_summary['æ•°é‡'] / len(results_df) * 100).round(1)
        sentiment_summary.to_excel(writer, sheet_name='æƒ…æ„Ÿ_æ‘˜è¦', index=False)
        
        # ìƒì„¸ ê²°ê³¼
        results_df.to_excel(writer, sheet_name='è¯¦ç»†_ç»“æœ', index=False)
    
    print(f"âœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_filename}")
    print(f"ğŸ“Š ì´ {len(final_df)} í–‰ì˜ ë°ì´í„°ê°€ ë¶„ì„ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # íŒŒì¼ ë‹¤ìš´ë¡œë“œ
    files.download(output_filename)
    
    # ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°
    print("\nğŸ” ìµœì¢… ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°:")
    print(final_df.head())
    
    return output_filename

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ‡¨ğŸ‡³ ì¤‘êµ­ì–´ ê°ì„±ë¶„ì„ ì‹œìŠ¤í…œ ì‹œì‘!")
    print("=" * 50)
    
    # 1. ëª¨ë¸ ë¡œë“œ
    tokenizer, model, device = load_chinese_model()
    
    # 2. Excel íŒŒì¼ ì—…ë¡œë“œ
    df, filename = upload_excel_file()
    if df is None:
        return
    
    # 3. í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì°¾ê¸°
    text_column = find_text_column(df)
    if text_column is None:
        return
    
    # 4. ê°ì„±ë¶„ì„ ì‹¤í–‰
    print("\nğŸ”„ ê°ì„±ë¶„ì„ ì‹¤í–‰ ì¤‘...")
    
    # í…ìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
    texts = df[text_column].dropna().astype(str).tolist()
    
    if texts:
        # ê°ì„±ë¶„ì„ ì‹¤í–‰
        results = analyze_sentiment(texts, tokenizer, model, device)
        
        # ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
        results_df = pd.DataFrame(results)
        
        print(f"âœ… ê°ì„±ë¶„ì„ ì™„ë£Œ: {len(results_df)} í…ìŠ¤íŠ¸ ë¶„ì„")
        
        # ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°
        print("\nğŸ” ê°ì„±ë¶„ì„ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°:")
        print(results_df.head())
        
        # ê°ì„± ë¶„í¬
        sentiment_counts = results_df['sentiment_label'].value_counts()
        print("\nğŸ“Š ê°ì„± ë¶„í¬:")
        for sentiment, count in sentiment_counts.items():
            percentage = (count / len(results_df)) * 100
            print(f"- {sentiment}: {count}ê°œ ({percentage:.1f}%)")
        
        # 5. ì‹œê°í™”
        print("\nğŸ“Š ì‹œê°í™” ìƒì„± ì¤‘...")
        create_visualizations(results_df)
        
        # 6. ê²°ê³¼ ì €ì¥
        print("\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘...")
        output_filename = save_results(df, results_df, filename)
        
        # 7. ìš”ì•½
        print("\n" + "=" * 50)
        print("ğŸ¯ ì¤‘êµ­ì–´ ê°ì„±ë¶„ì„ ì™„ë£Œ!")
        print(f"ğŸ“Š ì´ í…ìŠ¤íŠ¸ ìˆ˜: {len(results_df)}")
        print(f"ğŸ“Š í‰ê·  ì‹ ë¢°ë„: {results_df['sentiment_confidence'].mean():.3f}")
        print(f"ğŸ“ ë‹¤ìš´ë¡œë“œ íŒŒì¼: {output_filename}")
        print("=" * 50)
        
    else:
        print("âŒ ë¶„ì„í•  í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
